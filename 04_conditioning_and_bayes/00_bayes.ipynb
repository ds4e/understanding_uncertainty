{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa15a787",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conditioning and Bayes\n",
    "### Understanding Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb219ff7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction\n",
    "- Maximum Likelihood (plus confidence intervals, hypothesis tests, $p$-values, standard errors, sampling distribution) are the framework of **frequentist statistics**: We run experiments many times, and it reveals the reliability of our procedures\n",
    "- In frequentist statistics, we use those experiments to learn about a true, fixed $\\beta$ by way of estimates $\\hat{\\beta}_n$\n",
    "- In Bayesian statistics, $\\beta$ is assumed to be a random variable about which we hold explicit beliefs (a distribution): We use data as evidence to update those beliefs\n",
    "- Like with bootstrapping, the \"answer\" in Bayesian statistics is a \"posterior distribution\" after witnessing the data: The answer is typically a distribution, not a number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099f0379",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Roadmap\n",
    "1. Conditional Probability\n",
    "2. A Glimpse of Bayesian Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf20d491",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conditional Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64096458",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivating Example: Confusion Matrices and Classification Metrics\n",
    "- Let's use the metabric data to predict who is classified as alive or dead by the end of the sample period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8e1225",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('./data/metabric.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e125af4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(df['Tumor Size'].value_counts())\n",
    "df['Tumor Size'].plot.hist(bins=50)\n",
    "plt.show()\n",
    "\n",
    "df['size_log'] = np.log(df['Tumor Size'])\n",
    "df['size_log'].plot.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1919abd8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(df['Mutation Count'].value_counts())\n",
    "df['Mutation Count'].plot.hist(bins=50)\n",
    "plt.show()\n",
    "\n",
    "df['mut_log'] = np.log(df['Mutation Count'])\n",
    "df['mut_log'].plot.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ab155f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Clean Data:\n",
    "vars_num = df.loc[:, ['Age at Diagnosis', \n",
    "                     'Nottingham prognostic index', \n",
    "                     'TMB (nonsynonymous)',\n",
    "                     'size_log',\n",
    "                     'mut_log'] ]\n",
    "\n",
    "stage = pd.get_dummies(df['Tumor Stage'], drop_first=True, dtype=int)\n",
    "radio = pd.get_dummies(df['Radio Therapy'], drop_first=True, dtype=int)\n",
    "chemo = pd.get_dummies(df['Chemotherapy'], drop_first=True, dtype=int)\n",
    "type = pd.get_dummies(df['Type of Breast Surgery'], drop_first=True, dtype=int)\n",
    "\n",
    "y = df['Overall Survival Status'].copy().replace({'1:DECEASED':'0','0:LIVING':'1'})\n",
    "y = pd.to_numeric(y,errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29739ba8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Organize data:\n",
    "data = pd.concat([y, vars_num, stage, radio, chemo, type], axis=1)\n",
    "print(data.shape)\n",
    "data.dropna()\n",
    "print(data.shape)\n",
    "\n",
    "y = data['Overall Survival Status']\n",
    "X = data.drop('Overall Survival Status',axis=1)\n",
    "X = np.array(X)\n",
    "\n",
    "## Train-test split:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, # Feature and target variables\n",
    "                                                    test_size=.2, # Split the sample 80 train/ 20 test\n",
    "                                                    random_state=100) # For replication purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787cba5d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Fit a predictive model:\n",
    "reg = MLPClassifier(solver = 'adam',\n",
    "                        hidden_layer_sizes=(64), #, 32),\n",
    "                        activation='logistic',\n",
    "                        max_iter = 2000)\n",
    "reg = reg.fit(X_train,y_train)\n",
    "y_hat = reg.predict(X_test)\n",
    "print( 'Test Accuracy: ', reg.score(X_test,y_test) ) ## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89807d80",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Confusion matrix:\n",
    "tab = pd.crosstab(y_test,y_hat)\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b76c598",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9418de85",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Confusion Matrix\n",
    "- So, not a great model, but it tries. Out of 269 cases, \n",
    "    - 118 were correctly predicted as negative: **True Negatives (TN)**\n",
    "    - 62 were correctly predicted as positive: **True Positive (TP)**\n",
    "    - 58 were positive, but predicted negative: **False Negatives (FN)**\n",
    "    - 31 were negative, but predicted positive: **False Positive (FP)**\n",
    "- We wish to make sense of these numbers, but there are many definitions on which to analyze them\n",
    "- Do we care about the asymmetry between FP and FN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3146733",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Accuracy\n",
    "- Out of the total population, how many did we get right? This is called **accuracy**:\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP+TN}{N}\n",
    "$$\n",
    "- In this case, we get .699. Despite making a significant number of errors, about 70% of our predictions are correct.\n",
    "- In our frequentism world, we could get the sampling distribution. This is an estimate.\n",
    "- This estimates, **What is the (unconditional) probability this model makes a correct prediction?**\n",
    "- But this suggests a probabilistic interpretation of the table, as an estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bc0066",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Joint Density/Mass Function\n",
    "- If we divide the table by $N$, we get a **joint distribution**\n",
    "- Each cell says, \"If someone walks in the door, what are we likely to predict for them and are they likely to live or die?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f655a1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "N_test = len(y_test)\n",
    "joint = tab/N_test\n",
    "joint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05909687",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- So we have two random variables: The truth and our prediction. They have some stochastic relationship, summarized by the numbers in the table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3b2f23",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Marginal Distributions\n",
    "- If we sum over one of the variables, holding the other constant, we get a valid distribution function, called the **marginal distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32379668",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "np.sum(joint,axis=0) # Marginal of predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c55f728",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "np.sum(joint,axis=1) # Marginal of true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5330e7a5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- 55% actually die, 44% actually live."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd42df3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Marginal Distributions\n",
    "- We \"integrate out\" one of the variables to see how the other behaves on its own terms:\n",
    "$$\n",
    "f_Y(y) = \\begin{cases}\n",
    "\\sum_{x} f_{XY}(x,y), & \\text{ $X$ is discrete}\\\\\n",
    "\\int_{x} f_{XY}(x,y)dy, & \\text{ $Y$ is continuous}\n",
    "\\end{cases}\n",
    "$$\n",
    "- We're removing one of the variables from the situation to focus on the other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc0faf8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Conditional Distributions\n",
    "- What if we've seen the prediction... Now, is it accurate?\n",
    "- So imagine you've been told the test is positive. Are you *certain* you're going to die? Of course not. \n",
    "- What is the actual performance of the signal, on the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3c103f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Conditional Distributions\n",
    "- So, if I see a 0 prediction, I know it's only correct with probability\n",
    "$$\n",
    "\\text{Negative Predictive Value} = \\dfrac{TP}{TP+FP} \n",
    "$$\n",
    "and if I see a 1 prediction, I know it's only correct with probability\n",
    "$$\n",
    "\\text{Positive Predictive Value} = \\dfrac{TN}{TN+FN}\n",
    "$$\n",
    "- So, holding the column fixed, I compute the mass of each cell divided by total mass of the column\n",
    "$$\n",
    "\\mathbb{P}[0|\\hat{0}] = \\frac{\\mathbb{P}[0,\\hat{0}]}{\\mathbb{P}[\\hat{0}]}, \\quad \\mathbb{P}[1|\\hat{1}] = \\frac{\\mathbb{P}[1,\\hat{1}]}{\\mathbb{P}[\\hat{1}]}\n",
    "$$\n",
    "- These are hardly certain, despite the binary nature of the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7011f10d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "tab = np.array(tab)\n",
    "tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00514983",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "conditional_y_hat = tab/np.sum(tab,axis=1,keepdims=True)\n",
    "conditional_y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f63a5bb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Conditional Distributions\n",
    "- Likewise, we can ask, \"What if someone is NOT going to die? Does the model predict that?\" Then we want to compute\n",
    "$$\n",
    "\\text{Sensitivity} = \\dfrac{TP}{TP+FN} = \\dfrac{62}{62+58}\n",
    "$$\n",
    "- Conversely, \"What if someone is going to die? Does the model predict it correctly?\"\n",
    "$$\n",
    "\\text{Specificity} = \\dfrac{TN}{TN+FP} = \\dfrac{118}{118+31}\n",
    "$$\n",
    "- So, holding the row fixed, I compute the mass of each cell divided by total mass of the row\n",
    "$$\n",
    "\\mathbb{P}[\\hat{1}|1] = \\frac{\\mathbb{P}[1,\\hat{1}]}{\\mathbb{P}[1]}, \\quad \\mathbb{P}[\\hat{0}|0] = \\frac{\\mathbb{P}[0,\\hat{0}]}{\\mathbb{P}[0]}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c581a507",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d305083d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a970f35",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "conditional_y = tab/np.sum(tab,axis=1,keepdims=True)\n",
    "conditional_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e8966c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Who cares?\n",
    "- If you are a patient, anxious about the likelihood you actually live or die. Which do you want to know: Sensitivity/Specificity, or NPV/PPV?\n",
    "- If you are a doctor, deciding how to treat the patient. Which do you want to know: Sensitivity/Specificity, or NPV/PPV?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1061cdc3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Conditional Distributions\n",
    "- Now, re-normalize the joint probabilities by the conditioning event that $X=x$:\n",
    "$$\n",
    "f[y|X=x] = \\begin{cases}\n",
    "\\frac{f_{XY}(x,y)}{\\sum_{\\{x:X=x\\}} f_{XY}(x,y)}, & \\text{ $X$ is discrete}\\\\\n",
    "\\frac{f(x,y)}{f_X(x)}, & \\text{ $X$ is continuous}\n",
    "\\end{cases}\n",
    "$$\n",
    "- We're holding one variable constant, and seeing how the density/mass of the other changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3103ff1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Key Analytical Insight\n",
    "- If we have a multivariate context, we can **condition** on one variable to understand the other\n",
    "- We can always write\n",
    "$$\n",
    "\\underbrace{\\mathbb{P}[A \\cap B]}_{\\text{Joint}} = \\underbrace{\\mathbb{P}[A|B]}_{\\text{Conditional}} \\times \\underbrace{\\mathbb{P}[B]}_{\\text{Marginal}}\n",
    "$$\n",
    "$$\n",
    "\\underbrace{f_{YX}(y,x)}_{\\text{Joint}} = \\underbrace{ f_{Y|X}[y|x] }_{\\text{Conditional}} \\times \\underbrace{f_X(x)}_{\\text{Marginal}}\n",
    "$$\n",
    "- So, hold $B$ or $X=x$ constant and analyze $A$ or $Y$, then adjust $X=x$ to cover the range of possible values for $X$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59eac8d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Conditional Probability: Events\n",
    "- To understand conditional probability, let's dump the notation, then do examples: For two events $A$ and $B$, the probability of $A$ given that $B$ has occurred is\n",
    "$$\n",
    "\\mathbb{P}[A|B] = \\dfrac{\\mathbb{P}[A \\cap B]}{\\mathbb{P}[B]}\n",
    "$$\n",
    "- So the **probability of event $A$ given event $B$** is the probability of the intersection... normalized by the probability of $B$\n",
    "- We did this all the time at the beginning of class, with non-parametric estimation: \"Given we're in this bin/neighborhood, how many data points are there? How many satisfy our condition of interest?\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc27656d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Conditional Probability: Densities\n",
    "- In the case that we have a random variable $Y: Z|X \\rightarrow \\mathbb{R}$, we have\n",
    "$$\n",
    "f[y|X=x] = \\frac{f_{XY}(x,y)}{f_X(x)}\n",
    "$$\n",
    "- Notice that the denominator is a density, not a distribution or mass function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92876c83",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Wrapping Up\n",
    "- Just because you predict it, doesn't make it true\n",
    "- We can have uncertainty about parameters, not just the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3d619a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian Statistics and Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f366bfa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bayesian Inference\n",
    "- In frequentism -- exemplified by MLE -- parameters like $\\mu$ and $\\lambda$ are fixed, and we reason about their probable values by looking at data\n",
    "- In Bayesianism, we model the parameters like $\\mu$ and $\\lambda$ as explicitly random, with distributions called a **prior distribution**, that we typically select based on our subjective experience\n",
    "- We then use data to update our priors, yielding a **posterior distribution** that expresses our beliefs about the likely value of the parameters after seeing the data\n",
    "- I cannot possibly get through much of Bayesian reasoning: We want to focus on the basic ideas, and then you can read BDA3 or something comparable to get the tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943fbc09",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Set-up\n",
    "- So we have a situation. $X$ are the random data, and $\\beta$ are the random coefficients.\n",
    "\n",
    "    - We have a prior density $f_\\beta(\\beta)$ about the likely values of $\\beta$\n",
    "    - We have a likelihood $L(x,\\beta) = f[x|\\beta]$ that maps our parameters into the joint density of observing the data\n",
    "    - We have the marginal $f_X(x) = \\int_{\\beta'} f[x|\\beta']f_{\\beta}(\\beta') d\\beta'$\n",
    "\n",
    "- It turns out that we can write the posterior, $f[\\beta|X=x]$ in terms of the above quantities (which are all known and observable from data), which is Bayes' Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e7e087",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bayes' Rule\n",
    "- Recall the conditional density is given by\n",
    "$$\n",
    "f[\\beta|X=x] = \\dfrac{f(\\beta,x)}{f_X(x)},\n",
    "$$\n",
    "and similarly, $f(\\beta,x) = L(x,\\beta)f_\\beta(\\beta)$.\n",
    "- Then **Bayes' Rule** is\n",
    "$$\n",
    "f[\\beta|X=x] = \\dfrac{L(x,\\beta)f_\\beta(\\beta)}{f_X(x)}\n",
    "$$\n",
    "- Notice we used the defintion of conditional probability **twice**. Once to get that ratio, and once to convert the joint into the reverse conditional probability.\n",
    "- In words, \"the posterior is the likelihood times the prior, divided by the marginal\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46fa4ea",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Operationalizing Bayes' Rule\n",
    "- Let's rewrite that, with more parts labeled:\n",
    "$$\n",
    "\\underbrace{f[\\beta|X=x]}_{\\text{Posterior}} = \\dfrac{ \\underbrace{L(x,\\beta)}_{\\text{Likelihood}} \\quad \\times \\quad \\underbrace{f_\\beta(\\beta)}_{\\text{Prior}}}{\\underbrace{f_X(x)}_{\\text{Marginal}}}\n",
    "$$\n",
    "- So instead of the frequentist/bootstrap framework of examining the sampling distribution of a statistic, we do the thing you probably think is obvious: Look at data and update our beliefs about parameters \n",
    "- Notice how central the role of the likelihood still is: We're combining our frequentist approach with an additional piece, the prior over the parameters $\\theta$. Previously, we treated them as fixed features of reality and tried to calibrate them in our models to match observed outcomes.\n",
    "- What is the denominator? Just the marginal probability of observing the data, and it equals $f_X(x) = \\int_{\\beta'} L(x,\\theta') \\pi(\\theta')d\\theta'$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51cf519",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bayesian Modeling\n",
    "- It is not that different from MLE, in many ways:\n",
    "\n",
    "    0. Pick a likelihood that is appropriate for generating your data, the same as we did for MLE. Pick a prior and its parameters to express your subjective beliefs about the likely values of the likelihood.\n",
    "    1. Use Bayesian updating to transform your prior $\\pi(\\theta)$ into your posterior $p[\\theta|X=x]$\n",
    "    2. Use your posterior to conduct analyses (most likely $\\theta$, prediction intervals, plug-in estimates for decision models, etc.) \n",
    "\n",
    "- Step 1 is not typically handled analytically by formulas these days, except for some special cases. It is done computationally, using Stan or PyMC.\n",
    "- Under the hood, Bayesian inference is a computationally heavy field: We typically don't explicitly update the prior using formulas, we ask the computer to provide a sample of parameter draws from the posterior distribution (this is very similar to bootstrapping and the sampling distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd7a5ad",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Analytical Example: Bernoulli Likelihood with a Beta Prior\n",
    "- This is the classic introductory example\n",
    "- The data are $\\{0,1\\}$-valued draws from a Bernoulli distribution: $pr[y] = p^{y}(1-p)^{1-y}$ for $y \\in \\{0, 1\\}$\n",
    "- $0 \\le p \\le 1$ is now random: We don't know what it is, but we have a prior that it is distributed Beta, with \"known\" parameters $(\\alpha, \\beta)$:\n",
    "$$\n",
    "\\pi(p) = c p^{\\alpha-1} (1-p)^{\\beta-1}\n",
    "$$\n",
    "where $c$ is a constant what won't matter for our problem\n",
    "- When we multiply the prior times the likelihood, we get the posterior that\n",
    "$$\n",
    "\\underbrace{f[p|y]}_{\\text{posterior}} = \\dfrac{c p^{\\alpha+\\sum_i y_i -1 } (1-p)^{\\beta + n - k -1}}{pr[y]}\n",
    "$$\n",
    "- Staring at this implies the posterior must be a Beta distribution with parameters $\\alpha' = \\alpha+k$ and $\\beta' = \\beta + n-k$\n",
    "- That's it!\n",
    "- Many edge cases are possible, if you pick the prior and likelihood to have shapes that return the same functional form as the prior; many textbooks on Bayesian analysis are full of chapters of this kind of thing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbb3f51",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## PyMC\n",
    "- There are great options for Bayesian statistics and modeling: Stan and PyMC are popular and very high quality\n",
    "- We want to avoid the \"distribution/density games\" around Bayesian modeling: Pick a natural likelihood and an appropriate prior and go estimate computationally, rather than try to game out a perfect conjugacy relationship that gives you a cute posterior and closed-form solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedc6b1f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: Housing Prices\n",
    "- Let's predict (log-)house prices from house characteristics \n",
    "- The 00_bayes_pymc.py file in the repo has the results, and gives the flavor of a Bayesian analysis\n",
    "- Our basic model is $\\ln(p_i) = \\beta_0 + \\beta_1 \\text{age} + \\sigma \\varepsilon_i$\n",
    "- Our basic prior is that $\\beta_0$ and $\\beta_1$ are normally distributed with means of 0 and standard deviations of 10, and $\\sigma$ is \"half-normal\" with a standard deviation of 1\n",
    "- Roughly: Instead of a sampling distribution, we have a posterior distribution; the point estimates are similar and therefore point predictions will be similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3001aae1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Hardest Part to Understand\n",
    "- Computational Bayesian methods do not typically solve or optimize\n",
    "- Computational Bayesian methods instead try to sample from the posterior distribution, and give you a sample of draws to play with\n",
    "- You then do... non-parametric statistics (KDE/ECDF/LCLS) on the sample\n",
    "- So, if you thought the beginning of the class was a digression, I sympathize, but it's KDE/ECDF all the way down\n",
    "- The sample is drawn using Markov Chain Monte Carlo, a sophisticated method for generating samples from data-driven distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60d0c56",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MLE vs Bayesian\n",
    "- The intuition is, the posterior density and the sampling distribution are similar objects, and we use them in similar ways, but with very different interpretations\n",
    "- As the sample size grows, the Bayesian updating step will place more and more weight on the likelihood than the prior\n",
    "- This means that the Bayesian estimates converge to the MLE ones in large samples (this is called the Bernstein-von Mises theorem: The Bayesian posterior converges to the CLT-based frequentist limit) \n",
    "- Like most holy wars in academia, the differences between frequentist and Bayesian are slight, once you master both frameworks; it's mostly people who don't fully understand one or the other making all the noise about intellectual purity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643c1e6b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bayesian Advantages with Big Data\n",
    "- When the sample size is small, but our prior is strong\n",
    "- Hierarchical modeling, in which the data involve \"different layers of resolution\" (e.g. students/classes/schools/school districts, patients/doctors/wards/hospitals/HMOs, defendants/judges/counties/states)\n",
    "- The posterior need not be fully parametric, and can go right into decision support systems (Bayesian optimization)\n",
    "- Empirical Bayes: Use the Bayesian framework, but then estimate the parameters of the prior as if you're doing MLE, and don't worry about the philosophical contradictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635b6743",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "- So, we're done!\n",
    "- You have seen non-parametric statistics, frequentist statistics and bootstrapping, and Bayesian statistics, alongside a basic grounding in theoretical probability (probability space, random variable, density, distribution, expectation)\n",
    "- There is always more to know, but remember: You can always just bootstrap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
